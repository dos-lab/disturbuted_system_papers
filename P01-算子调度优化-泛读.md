# 结合编译的单个算子放置

//机制：EAGLE使用强化学习和RNN模型确定算子设备放置时的最优算子分组。能够扩展已有强化学习的局限性，对比已有单个GPU、Tensorflow-Slim、层次规划和联合学习方法，能够取得较好效果。作为模型驱动的一种，能相交于启发式规则取得更好效果。
@inproceedings{lan2021eagle,
  title={EAGLE: Expedited Device Placement with Automatic Grouping for Large Models},
  author={Lan, Hao and Chen, Li and Li, Baochun},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={599--608},
  year={2021},
  organization={IEEE}
}


//机制：DUET首先使用通信感知的算子计算图划分方法减少计算子图之间的通信开销，然后使用性能检测工具监测这些子图的性能，最后使用贪心方法充分交叠CPU和GPU上的并行算子，最终克服当前放置方法不能充分利用异构资源并行加速比差异大的算子问题。实验与TVM、TensorFlow和Pytorch对比，从端到端宽模型并行执行延迟时间、调度方法的启发规则（贪心，装箱，随机，全局最优）、不同层的加速比和普通模型四个方面对比了本方法的有效性。
@inproceedings{zhang2021duet,
  title={DUET: A Compiler-Runtime Subgraph Scheduling Approach for Tensor Programs on a Coupled CPU-GPU Architecture},
  author={Zhang, Minjia and Hu, Zehua and Li, Mingqin},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={151--161},
  year={2021},
  organization={IEEE}
}

//模型：Mistify针对DL模型的设计、部署两个阶段差异较大，且部署在端云边环境效率较低的问题，设计了两层抽象，避免上层DL模型改变导致的频繁编译代码更新，使用基于树结构的整体算子运行环境适配以高效支持算子在云边环境下的放置，同时使用知识蒸馏方式提升设备间的通信效率，最后使用流式处理加速DL推理服务的响应时间，相交于传统部署方式能提升10倍的设计+部署时间。
@inproceedings{guo2021mistify,
  title={Mistify: Automating DNN Model Porting for On-Device Inference at the Edge.},
  author={Guo, Peizhen and Hu, Bo and Hu, Wenjun},
  booktitle={NSDI},
  pages={705--719},
  year={2021}
}

## 通过算子计算图替换的方式来间接实现算子的融合替换

//面向特定的算子组合，根据数学原则，实行严格的等价替换，以提升算子性能。
@inproceedings{jia2019taso,
  title={TASO: optimizing deep learning computation with automatic generation of graph substitutions},
  author={Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={47--62},
  year={2019}
}

//相较于TASO，进行非等价替换。这两类方法和算子编译优化方法并不冲突，可以作为不同的优化方向。
@inproceedings{wang2021pet,
  title={$\{$PET$\}$: Optimizing Tensor Programs with Partially Equivalent Transformations and Automated Corrections},
  author={Wang, Haojie and Zhai, Jidong and Gao, Mingyu and Ma, Zixuan and Tang, Shizhi and Zheng, Liyan and Li, Yuanzhi and Rong, Kaiyuan and Chen, Yuanyong and Jia, Zhihao},
  booktitle={15th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 21)},
  pages={37--54},
  year={2021}
}

//该方法针对TASO等子图替换规则不能搜索到所有可能替换空间的问题，使用编译等价饱和技术，可针对一个完整的计算图搜索到所有的替换子图，并选择最优子图作为优化结果。
@article{yang2021equality,
  title={Equality saturation for tensor graph superoptimization},
  author={Yang, Yichen and Phothilimthana, Phitchaya and Wang, Yisu and Willsey, Max and Roy, Sudip and Pienaar, Jacques},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  year={2021}
}

## 融合单个DL模型内部的相同算子

//模型：首次将延迟调度的思路引入算子粒度的调度，发现并行计算算子在不同执行时机对模型的数据流图有较大影响，并使用递归计算状态可变数据流子图最短完成时间达到整个大图的完成时间优化。该文献重点讨论了一个数据流内算子不同执行时机造成的影响，未来需要进一步考虑异构资源容量、通信开销、跨数据资源共享时对算子延迟调度造成的影响，或者一些新的共享效率提升潜力。此外，单一算子调度在50%的情况下并不能得到比TVM优化后更快的优化速度，未来可能需要两者结合，同时需要与rammer进行区分（overhead等方面考虑）。
@article{ding2021ios,
  title={Ios: Inter-operator scheduler for cnn acceleration},
  author={Ding, Yaoyao and Zhu, Ligeng and Jia, Zhihao and Pekhimenko, Gennady and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  year={2021}
}



## 多个相同模型自身重复运行时相同算子融合

//机制：集群环境中存在多种使用单卡加速的DL模型，这些模型存在相同算子（处理逻辑，输入输出形状），进行跨模型的算子融合能够提升资源利用率和GPU、TPU的吞吐量；HFTA还同时提出对应的训练时超参数更新方法，从而保证收敛性。其适用于多个相似DL模型使用单卡资源反复执行时资源利用率提升的问题。
@article{wang2021horizontally,
  title={Horizontally Fused Training Array: An Effective Hardware Utilization Squeezer for Training Novel Deep Learning Models},
  author={Wang, Shang and Yang, Peiming and Zheng, Yuxuan and Li, Xin and Pekhimenko, Gennady},
  journal={arXiv preprint arXiv:2102.02344},
  year={2021}
}

//对不同的层次中的算子进行融合，充分评估的多个DL模型的内存占用开销，使用动态规划方法优化资源使用
@inproceedings{cai2021optimus,
  title={Optimus: towards optimal layer-fusion on deep learning processors},
  author={Cai, Xuyi and Wang, Ying and Zhang, Lei},
  booktitle={Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
  pages={67--79},
  year={2021}
}

//循环融合算子
@inproceedings{niu2021dnnfusion,
  title={DNNFusion: accelerating deep neural networks execution with advanced operator fusion},
  author={Niu, Wei and Guan, Jiexiong and Wang, Yanzhi and Agrawal, Gagan and Ren, Bin},
  booktitle={Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  pages={883--898},
  year={2021}
}

## 对关键路径上的算子进行切分融合，细粒度算子切分

//策略：DL模型基于数据并行方式，长期运行时会面临通信开销大导致的伸缩性问题，本文提出使用分布式SGD算法提升算子状态同步的效率，使用算子稀疏矩阵的并行切分运行缩短完成时间，能够对典型CNN网络进行分布式加速。此文论证了细粒度的算子并行状态同步能相交于粗粒度的数据并行，能减少通信开销从而保障更好的CNN训练伸缩性。
@inproceedings{demirci2021partitioning,
  title={Partitioning sparse deep neural networks for scalable training and inference},
  author={Demirci, Gunduz Vehbi and Ferhatosmanoglu, Hakan},
  booktitle={Proceedings of the ACM International Conference on Supercomputing},
  pages={254--265},
  year={2021}
}

//机制：FastT将MS-DF简化为有向无环图结构，通过静态分析感知算子执行时的关键路径，并对关键路径上的算子依次进行调度，最后切分关键路径上的部分算子，以进一步缩短训练完成时间。
@inproceedings{yi2020fast,
  title={Fast Training of Deep Learning Models over Multiple GPUs},
  author={Yi, Xiaodong and Luo, Ziyue and Meng, Chen and Wang, Mengdi and Long, Guoping and Wu, Chuan and Yang, Jun and Lin, Wei},
  booktitle={Proceedings of the 21st International Middleware Conference},
  pages={105--118},
  year={2020}
}

//机制：MS-DF的任务需要其所处作业内的调度才能执行，任务中每个算子的调度则往往通过第三方的深度学习库来进行，这2级调度器之间实际资源使用的差异往往会导致资源浪费，Rammer即据此设计了算子集合任务和异构可用资源单元这两类抽象，在编译阶段进行算子内和算子间的调度优化，避免调度的开销；同时对GPU、TPU和IPU等资源进行统一接口的抽象。可通过算子内并行计划和算子间约束关系最小化运行时间。
@inproceedings{ma2020rammer,
  title={Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks},
  author={Ma, Lingxiao and Xie, Zhiqiang and Yang, Zhi and Xue, Jilong and Miao, Youshan and Cui, Wei and Hu, Wenxiang and Yang, Fan and Zhang, Lintao and Zhou, Lidong},
  booktitle={14th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 20)},
  pages={881--897},
  year={2020}
}

//策略：在CPU的PS DL训练环境下，Harmony分析出单个训练作业分为拉取、计算和推送三个过程，面向多个训练作业，交叠这三个阶段，可以显著提升资源使用效率。其与Salus等工作的上GPU复用方法类似。未来可以在异构计算资源的细粒度精确共享方面进一步提升资源使用效率，从而可以缩短DL训练、推理时间。
@inproceedings{lee2021harmony,
  title={Harmony: A Scheduling Framework Optimized for Multiple Distributed Machine Learning Jobs},
  author={Lee, Woo-Yeon and Lee, Yunseong and Song, Won Wook and Yang, Youngseok and Kim, Joo Yeon and Chun, Byung-Gon},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)},
  pages={841--851},
  year={2021},
  organization={IEEE}
}

## 算子自身资源调整

//机制：使用爬山算法预测算子在CPU上的完成时间
@inproceedings{liu2019runtime,
  title={Runtime concurrency control and operation scheduling for high performance neural network training},
  author={Liu, Jiawen and Li, Dong and Kestor, Gokcen and Vetter, Jeffrey},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={188--199},
  year={2019},
  organization={IEEE}
}

//算子使用GPU资源时面临cuda core 之间的cache竞争和冲突问题，其重新设计了cache算法，进一步提升了算子使用资源时的存储资源开销。其可以辅助用于算子的编译优化。
@inproceedings{ibrahim2020analyzing,
  title={Analyzing and Leveraging Shared L1 Caches in GPUs},
  author={Ibrahim, Mohamed Assem and Kayiran, Onur and Eckert, Yasuko and Loh, Gabriel H and Jog, Adwait},
  booktitle={Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques},
  pages={161--173},
  year={2020}
}

## DL模型内不同算子的融合

//机制：现有算子融合方法仅是简单将不同算子运行逻辑端到端整合到一起，缺乏根据其自身运行逻辑的深度融合，本文将Relu作为mask过滤器与conv2D算子进行深度融合，预存权重与relu处理后结果，从而极大加速小模型的推理速度，同时层算子融合后也减少了算子数量。本方法适用于小模型同构资源的推理场景。
@inproceedings{olyaiy2021accelerating,
  title={Accelerating DNNs inference with predictive layer fusion},
  author={Olyaiy, MohammadHossein and Ng, Christopher and Lis, Mieszko},
  booktitle={Proceedings of the ACM International Conference on Supercomputing},
  pages={291--303},
  year={2021}
}

## 单个编译优化手段

//策略：收集不同算子的性能数据，构造数据库进行高效检索，同时面向新负载中的新算子，使用autoML，采用近似最优的schedule调整方法得到优化后的算子性能表现。此种方法不涉及融合、切分等在异构资源上的多种编译优化手段，未来可以围绕算子细粒度性能建模、编译优化使用异构资源方面持续发展。
@inproceedings{yu2021lorien,
  title={Lorien: Efficient Deep Learning Workloads Delivery},
  author={Yu, Cody Hao and Shi, Xingjian and Shen, Haichen and Chen, Zhi and Li, Mu and Wang, Yida},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={18--32},
  year={2021}
}

//机制：分析不同算子在GPU和FPGA上的性能表现，发现GPU在不同输入下的功率波动比FPGA大，FGPA一般比GPU耗能低，以及有些算子在FPGA上跑的更快，基于此确定某些算子的调度；FPGA在运行时需要调整CU、SIMID、循环程度，组大小等FPGA参数，并根据这些参数构造性能模型；在实际运行时，重点最小化所有异构设备的性能开销和最大化训练性能（吞吐量）。基于这三个方面，实验验证了在多块CPU、FGPA组合情况下的表现。此种方法缺乏对GPU等编译层面的优化，可以与其他工作进行结合以达到更好的效果；同时，也可以提升GPU、FPGA的并行度优化。
@inproceedings{he2021enabling,
  title={Enabling energy-efficient DNN training on hybrid GPU-FPGA accelerators},
  author={He, Xin and Liu, Jiawen and Xie, Zhen and Chen, Hao and Chen, Guoyang and Zhang, Weifeng and Li, Dong},
  booktitle={Proceedings of the ACM International Conference on Supercomputing},
  pages={227--241},
  year={2021}
}

//机制：本方法针对CNN中的主要瓶颈算子，卷积，使用费雪检验降低可能的算子编译优化空间，并使用多面体编译优化技术得到最优的执行序列。其实验与TVM默认的多种编译优化方法相比，能提升3倍的性能，未来可以在算子同时使用异构资源时，对非卷子算子和更多的优化手段相比，提升其适用范围。
@inproceedings{turner2021neural,
  title={Neural architecture search as program transformation exploration},
  author={Turner, Jack and Crowley, Elliot J and O'Boyle, Michael FP},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={915--927},
  year={2021}
}

## CPU/GPU性能建模与分析优化

//机制：分析了NVIDIA GPU 采用most-room的线程调度策略，而不是round-robin，此种方法忽略SM的干扰，存在资源竞争现象。
@article{gilman2021demystifying,
  title={Demystifying the Placement Policies of the NVIDIA GPU Thread Block Scheduler for Concurrent Kernels},
  author={Gilman, Guin and Ogden, Samuel S and Guo, Tian and Walls, Robert J},
  journal={ACM SIGMETRICS Performance Evaluation Review},
  volume={48},
  number={3},
  pages={81--88},
  year={2021},
  publisher={ACM New York, NY, USA}
}

//机制：在DL负载下，通过优先级stream,MPS和分时复用三种GPU并行方式观察GPU被DL训练和推理服务共享时的性能表现，讨论影响共享性能和资源使用效率的关键因素，为后续DL负载的GPU调度提供基础
@article{gilman2021characterizing,
  title={Characterizing concurrency mechanisms for NVIDIA GPUs under deep learning workloads},
  author={Gilman, Guin and Walls, Robert J},
  journal={Performance Evaluation},
  volume={151},
  pages={102234},
  year={2021},
  publisher={Elsevier}
}

//Smaug基于端到端的思想充分评估DL作业的性能开销，基于gem5-Aladdin进行软硬件资源的模型，并通过实现ACP接口提升数据转移性能、使用多个加速器提升DL速度以及使用多线程加速数据处理过程三个措施提升DL的加速效果。此文表明DL会受到多种因素影响，需要充分评估应用多个阶段和不同层次的加速工作后才能提升DL训练、推理性能。
@article{xi2020smaug,
  title={Smaug: End-to-end full-stack simulation infrastructure for deep learning workloads},
  author={Xi, Sam and Yao, Yuan and Bhardwaj, Kshitij and Whatmough, Paul and Wei, Gu-Yeon and Brooks, David},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={17},
  number={4},
  pages={1--26},
  year={2020},
  publisher={ACM New York, NY, USA}
}

//此方法将DL模型切分不同的阶段，在各个阶段内通过动态优化方法调整其运行时间，以最大化吞吐量；然后在每个阶段内部，通过feature map算子的切分、对工作负载的分析等方式适应多种异构设备。此种方法只适用于CNN环境下多个负载的受限算子，且假设负载是泊松分布，存在一定的局限性。
@inproceedings{yang2021towards,
  title={Towards Efficient Inference: Adaptively Cooperate in Heterogeneous IoT Edge Cluster},
  author={Yang, Xiang and Qi, Qi and Wang, Jingyu and Guo, Song and Liao, Jianxin},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)},
  pages={12--23},
  year={2021},
  organization={IEEE}
}

//Tailored Profiling关注算子运行图的性能，从代码书写、编译、IR和机器代码多个层次上profile运行图性能，其核心是构造映射词典，实现代码、计算图、算子、IR和指令的多层关联，方便领域专家、开发人员和性能调优人员使用并改善性能；其在Umbra工具中进行了集成，且支持算子融合、常量折叠等等方面的性能建模，其profile开销和准确度也进行了验证。未来可以在异构资源和特定深度学习领域进行持续优化。
@inproceedings{beischl2021profiling,
  title={Profiling dataflow systems on multiple abstraction levels},
  author={Beischl, Alexander and Kersten, Timo and Bandle, Maximilian and Giceva, Jana and Neumann, Thomas},
  booktitle={Proceedings of the Sixteenth European Conference on Computer Systems},
  pages={474--489},
  year={2021}
}

//当前基于特定硬件上的深度学习库加速不能支持所有算子的问题，以及基于搜索/机器学习方法进行算子编译优化存在较大性能开销的问题，Tuna使用异构特定的建模方法，为不同资源构造特定的性能优化费用模型。其中CPU优化模型主要关心SIMD指令数量，L1 cache命中率，指令并行能力。GPU主要关心PTX指令数、线程级并行数量、SM占用数量，warp调度、共享显存冲突情况。基于这些费用指标，指导进行高效搜索，能提升1.54倍的性能。这些启发式规则能相较于原生TVM实现更好效果，但存在为异构资源分别建模的代价，也不支持异构资源同时使用的情况，同时，这些规则和编译优化手段缺乏直接的关联，不能显式评估各个优化手段对最终效果的提升能力。
@article{wang2021tuna,
  title={Tuna: A static analysis approach to optimizing deep neural networks},
  author={Wang, Yao and Zhou, Xingyu and Wang, Yanming and Li, Rui and Wu, Yong and Sharma, Vin},
  journal={arXiv preprint arXiv:2104.14641},
  year={2021}
}

//该方法总结当前基于搜索（机器学习cost模型）、特定厂商的性能优化包和多面体编译优化的三种方法，都难以搜索出最优的tile结果，其建模CPU分层存储、缓存的方法，直接分析不同编译schedule对特定CPU存储模型的影响，是首个基于资源分析的方法确定最优的编译结果，相较于当前盲目的搜索和厂商特定编译均能取得更好的效果。异构资源同时使用、多种编译优化手段选择等问题，仍然可以进一步研究。
@inproceedings{li2021analytical,
  title={Analytical characterization and design space exploration for optimization of CNNs},
  author={Li, Rui and Xu, Yufan and Sukumaran-Rajam, Aravind and Rountev, Atanas and Sadayappan, P},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={928--942},
  year={2021}
}

//策略：主要分析了NLP DL模型中算子的特点，包括算子依赖的多分支、计算和通信不能有效交叠这三方面， lbann使用数据与算子子图混合并行的方式提升transform模型的伸缩性。
@inproceedings{jain2021super,
  title={SUPER: SUb-Graph Parallelism for TransformERs},
  author={Jain, Arpan and Moon, Tim and Benson, Tom and Subramoni, Hari and Jacobs, Sam Ad{\'e} and Panda, Dhabaleswar K and Van Essen, Brian},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={629--638},
  year={2021},
  organization={IEEE}
}

//机制：Mopt从卷积算子计算逻辑的循环优化出发，指出张量不同循环层次的“贴片”使用不同内存（缓存）的情况是影响其自身执行时间的关键，其通过L1，L2，L3和内存之间数据移动的分析，建模数据传输的性能开销，简化最优贴片和变形最优配置的决策复杂度，能刻画传统黑盒方法受限搜索空间的局限性，相较于TVM能最多提升2倍的加速比。Mopt适用场景较为有限，目前只支持CPU和卷积算子，可以与TVM结合进一步提升其适用范围。
@inproceedings{li2021analytical,
  title={Analytical characterization and design space exploration for optimization of CNNs},
  author={Li, Rui and Xu, Yufan and Sukumaran-Rajam, Aravind and Rountev, Atanas and Sadayappan, P},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={928--942},
  year={2021}
}
## 框架工具

//TC通过深度学习编译的领域特定语言、多面底层IR表达式、schedule调整、贴片优化、线程块绑定、内存缓存优化、第三方NN库调用、配置自动调整和缓存等方面提供对DNN的全量支持能力，相较于TVM等其能力更加全面，但缺乏大规模的应用和开发。
@article{vasilache2019next,
  title={The next 700 accelerated layers: From mathematical expressions of network computation graphs to accelerated GPU kernels, automatically},
  author={Vasilache, Nicolas and Zinenko, Oleksandr and Theodoridis, Theodoros and Goyal, Priya and Devito, Zachary and Moses, William S and Verdoolaege, Sven and Adams, Andrew and Cohen, Albert},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={16},
  number={4},
  pages={1--26},
  year={2019},
  publisher={ACM New York, NY, USA}
}

//Mlir作为一种IR表达，与halide类似，可与TVM等编译优化框架耦合，提供了DL编译时IR的低资源开销，可追踪可持续演进等能力。
@inproceedings{lattner2021mlir,
  title={Mlir: Scaling compiler infrastructure for domain specific computation},
  author={Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  booktitle={2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={2--14},
  year={2021},
  organization={IEEE}
}

//提出了一种跨域的IR表达方法，能够支持机器人、图分析、深度学习等多个领域的泛化IR表达方法，也能够支持不同算子计算粒度的的表示和异构资源计算的表示，其同样采用基于流水线的编译过程，能够被tvm等框架所整合，但目前缺乏基于PolyMath的编译优化机制。
@inproceedings{kinzer2021computational,
  title={A Computational Stack for Cross-Domain Acceleration},
  author={Kinzer, Sean and Kim, Joon Kyung and Ghodrati, Soroush and Yatham, Brahmendra and Althoff, Alric and Mahajan, Divya and Lerner, Sorin and Esmaeilzadeh, Hadi},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={54--70},
  year={2021},
  organization={IEEE}
}

//其提出了2d-operator的表示方法，可以基于这种方式，构造多种复杂的算子，说明算子是DL中最重要的基本构成单位，对其进行表示和优化具有重要意义。
@article{georganas2021tensor,
  title={Tensor processing primitives: a programming abstraction for efficiency and portability in deep learning workloads},
  author={Georganas, Evangelos and Kalamkar, Dhiraj and Avancha, Sasikanth and Adelman, Menachem and Anderson, Cristina and Breuer, Alexander and Bruestle, Jeremy and Chaudhary, Narendra and Kundu, Abhisek and Kutnick, Denise and others},
  journal={arXiv preprint arXiv:2104.05755},
  year={2021}
}

//hummingbird能将传统ML模型编译成算子/张量计算图，以充分利用Pytorch、TVM的优化加速性能，其验证在pageRank等若干经典模型上的有效性和性能。未来，算子编译优化需要结合张量稀疏性、非数值变量计算、交叉编译、异构资源上进行进一步发展，才能较好应对数据处理过程中的低效计算、CPU/GPU移动等方面的开销。
@article{koutsoukos2021tensors,
  title={Tensors: An abstraction for general data processing},
  author={Koutsoukos, Dimitrios and Nakandala, Supun and Karanasos, Konstantinos and Saur, Karla and Alonso, Gustavo and Interlandi, Matteo},
  journal={Proceedings of the VLDB Endowment},
  volume={14},
  number={10},
  pages={1797--1804},
  year={2021},
  publisher={VLDB Endowment}
}

//AKG与TVM类似，重点提供对Huawei NPU的支持，其可以在代码量较少的情况下提供和TVM优化后相似的性能表现。
@inproceedings{zhao2021akg,
  title={AKG: automatic kernel generation for neural processing units using polyhedral transformations},
  author={Zhao, Jie and Li, Bojie and Nie, Wang and Geng, Zhen and Zhang, Renwei and Gao, Xiong and Cheng, Bin and Wu, Chen and Cheng, Yun and Li, Zheng and others},
  booktitle={Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  pages={1233--1248},
  year={2021}
}

## 编译之上优化（优化DL的运行逻辑、包括prune、multi-exit、sparisity等）

//该方法在模型层面通过设置多个DL的结束运行出口避免无效的计算，然后在计算过程中的端云边设备使用多级队列控制计算作业，避免资源的无效占用或浪费。这两个方面的联合优化能实现1-17倍的加速。
@inproceedings{huang2021enabling,
  title={Enabling Low Latency Edge Intelligence based on Multi-exit DNNs in the Wild},
  author={Huang, Zhaowu and Dong, Fang and Shen, Dian and Zhang, Junxue and Wang, Huitian and Cai, Guangxing and He, Qiang},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)},
  pages={729--739},
  year={2021},
  organization={IEEE}
}

//该方法从待训练、推理的数据出发，通过分类复杂度和实例复杂度感知的两类方法，决定哪些数据在端边上进行直接计算，哪些需要传输到云端进行进一步运算。此方法只适用于有监督学习的DL场景，当数据不具备这些数据类型时，其方法难以有效适用。
@inproceedings{long2021complexity,
  title={Complexity-aware Adaptive Training and Inference for Edge-Cloud Distributed AI Systems},
  author={Long, Yinghan and Chakraborty, Indranil and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)},
  pages={573--583},
  year={2021},
  organization={IEEE}
}

//CNN中的稀疏矩阵乘法进行剪枝时，存在精确度和资源使用效率之间的矛盾性，本文基于稀疏稠密矩阵和稠密矩阵间的乘法加速设计，同时使用结构化和非结构化剪枝，在提升资源效率的同时保持推理精确度。
@inproceedings{rumi2020accelerating,
  title={Accelerating sparse cnn inference on gpus with performance-aware weight pruning},
  author={Rumi, Masuma Akter and Ma, Xiaolong and Wang, Yanzhi and Jiang, Peng},
  booktitle={Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques},
  pages={267--278},
  year={2020}
}

//2PGraph依次使用数据并行和模型并行方法，实现对图数据的高效切分、采样和计算。其中，在切分和采样阶段，使用本地感知的规则降低采样的数据量，从而充分交叠计算和采样；在实际计算过程中，基于L临近方法进行高效缓存（缓存命中率达到100%），避免特征数据的频繁通信。实验在单个GPU、多个GPU、分布式训练、缓存优化效果和训练收敛速度这几个方面比较的工作的有效性。
@inproceedings{zhang20212pgraph,
  title={2PGraph: Accelerating GNN Training over Large Graphs on GPU Clusters},
  author={Zhang, Lizhi and Lai, Zhiquan and Li, Shengwei and Tang, Yu and Liu, Feng and Li, Dongsheng},
  booktitle={2021 IEEE International Conference on Cluster Computing (CLUSTER)},
  pages={103--113},
  year={2021},
  organization={IEEE}
}


## 综合使用多种手段进行算子优化编译

//机制：基于XLA，XTAT聚焦深度学习算子编译的多个阶段，基于性能模型评估各个编译阶段对性情造成的影响，选择最优的算子融合、算子贴片和重布局手段，提升5%-15%不等的推理性能。其在TPU上进行了实现，由于只考虑同构资源，在150个模型中，实际提升效果不高。结合异构资源性能，能够从多个维度优化算子性能仍然存在挑战。
@inproceedings{phothilimthana2021flexible,
  title={A Flexible Approach to Autotuning Multi-Pass Machine Learning Compilers},
  author={Phothilimthana, Phitchaya Mangpo and Sabne, Amit and Sarda, Nikhil and Murthy, Karthik Srinivasa and Zhou, Yanqi and Angermueller, Christof and Burrows, Mike and Roy, Sudip and Mandke, Ketan and Farahani, Rezsa and others},
  booktitle={2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  pages={1--16},
  year={2021},
  organization={IEEE}
}

//策略：XTAT-M提出高效算子编译优化的主要手段，考虑到不同编译优化阶段间可能造成的互相影响，并基于这种影响刻画优化费用函数，能够实施多个编译阶段联合的优化手段，扩展编译优化的搜索空间。其平均优化效果能在TPU v3上提升约5%。目前，基于算子的编译优化面临异构资源同时协同优化优化的问题，其优化手段也不仅限于文中提到的fusion、layout和tile三种，且这些方法的cost模型仅考虑了特定优化手段对最终推理性能的提升效果，而忽略了编译优化过程中的编译手段搜索开销。
@inproceedings{phothilimthana2021flexible,
  title={A Flexible Approach to Autotuning Multi-Pass Machine Learning Compilers},
  author={Phothilimthana, Phitchaya Mangpo and Sabne, Amit and Sarda, Nikhil and Murthy, Karthik Srinivasa and Zhou, Yanqi and Angermueller, Christof and Burrows, Mike and Roy, Sudip and Mandke, Ketan and Farahani, Rezsa and others},
  booktitle={2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  pages={1--16},
  year={2021},
  organization={IEEE}
}

//策略：DaCeML提供相对完备的深度学习模型优化可视化工具，组织为若干特定阶段，提供算子融合等方面的优化能力。
@article{rausch2021data,
  title={A Data-Centric Optimization Framework for Machine Learning},
  author={Rausch, Oliver and Ben-Nun, Tal and Dryden, Nikoli and Ivanov, Andrei and Li, Shigang and Hoefler, Torsten},
  journal={arXiv preprint arXiv:2110.10802},
  year={2021}
}

//TurboTransformers为了提升Transformer模型在GPU上的推理性能，使用算子融合方法，重点融合非矩阵乘法算子；在内存管理方法，使用局部/全局张量感知和计算拓扑图感知的方法，优化GPU的显存使用；实际线上推理过程中，使用动态规划优化保障QoS，以确定在不同负载下特定的batchsize选择。实验效果能实现运行时间和资源使用效率的较大提升。
@inproceedings{fang2021turbotransformers,
  title={TurboTransformers: an efficient GPU serving system for transformer models},
  author={Fang, Jiarui and Yu, Yang and Zhao, Chengduo and Zhou, Jie},
  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={389--402},
  year={2021}
}

//在CPU环境中，此方法在高层算子优化充分利用CPU的多层缓存，在底层优化时使用强化学习方法确定CPU SIMD的向量长度设置，其方法相较于原生的autoTVM方法有一定提升
@article{tavarageri2021ai,
  title={AI Powered Compiler Techniques for DL Code Optimization},
  author={Tavarageri, Sanket and Goyal, Gagandeep and Avancha, Sasikanth and Kaul, Bharat and Upadrasta, Ramakrishna},
  journal={arXiv preprint arXiv:2104.05573},
  year={2021}
}

//策略：DaCeML作为基于状态可变数据流进行编译优化的工具，首先将主流深度学习框架Tensorflow、pytorch和TVM等的代码或者onnx模型转化为自定义的IR表示方式，能够以数据转移为核心分析编译性能优化空间；在粗粒度编译优化阶段，综合使用常量折叠、融合以及部分算子的忽略提升标新性能；在本地数据移动编译优化阶段，使用子图融合，融合空间搜索等方式降低数据移动开销，其他如反向传播、整体数据布局优化等也做了优化。该工具可以应用于异构资源上，用于对比基于规则的融合效果。
@article{rausch2021data,
  title={A Data-Centric Optimization Framework for Machine Learning},
  author={Rausch, Oliver and Ben-Nun, Tal and Dryden, Nikoli and Ivanov, Andrei and Li, Shigang and Hoefler, Torsten},
  journal={arXiv preprint arXiv:2110.10802},
  year={2021}
}